---
layout: post
title:  "反向传播(BP)算法的推导"
date:   2019-07-24 20:22:52+0800
categories: DL 深度学习 BP 反向传播
---

## 优化任务的一般描述
一个估计器可以描述为如下形式. 其中$$\mathbf \Theta$$是参数集合, $$\mathbf x$$和$$\mathbf{\hat y}$$分别是输入和输出.

$$\mathbf{\hat y} = f(\mathbf x; \mathbf \Theta)$$

而学习目标则要求最小化损失函数$$J$$

$$\min_{\mathbf \Theta} J(\mathbf \Theta; \mathbf x; \mathbf y)$$

如果$$J$$是比较简单的形式, 如感知机中的均方误差MSE, 那么可以将$$J$$视为$$\theta$$的函数, 直接求导后使用梯度下降进行最优化求解. 下式中的$$\lambda$$为学习率.

$$\theta \leftarrow \theta - \lambda \frac{\partial J(\theta)}{\partial \theta}, \quad \forall \theta \in \mathbf \Theta$$

> 为方便理解, 可以将$$\theta$$视为自变量, 而$$\mathbf x$$和$$\mathbf y$$视为损失函数的参数.

但是如果$$J$$是比较复杂的形式, 比如是某个$$\theta \in \Theta$$的复合函数且对每个参数$$\theta$$的复合层级并不一致, 那么无法直接求得$$J$$对$$\theta$$的偏导数. 多层神经网络就是一个例子.

## BP算法一般形式的推导

考虑如下图所示的神经元

![一个神经元的输出被多个不同分支的神经元作为输入]()


