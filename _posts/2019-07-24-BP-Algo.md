---
layout: post
title:  "反向传播(BP)算法的推导"
date:   2019-07-24 20:22:52+0800
categories: DL 深度学习 BP 反向传播
---

## 优化任务的一般描述
一个估计器可以描述为如下形式. 其中$$\mathbf \Theta$$是参数集合, $$\mathbf x$$和$$\mathbf{\hat y}$$分别是输入和输出.

$$\mathbf{\hat y} = f(\mathbf x, \mathbf \Theta)$$

而学习目标则要求最小化损失函数$$J$$

$$\min_{\mathbf \Theta} J(\mathbf \Theta, \mathbf x, \mathbf y)$$

如果$$J$$是比较简单的形式, 如感知机中的均方误差MSE, 那么可以将$$J$$视为$$\theta$$的函数, 直接求导后使用梯度下降进行最优化求解. 下式中的$$\lambda$$为学习率.

$$\theta \leftarrow \theta - \lambda \nabla_\theta J(\theta), \quad \forall \theta \in \mathbf \Theta$$

> 为方便理解, 可以将$$\theta$$视为自变量, 而$$\mathbf x$$和$$\mathbf y$$视为损失函数的参数.

但是如果$$J$$是比较复杂的形式, 比如是某个$$\theta \in \Theta$$的复合函数, 甚至对每个参数$$\theta$$的复合层级并不一致, 那么无法直接求得$$J$$对$$\theta$$的偏导数. 多层神经网络就是一个例子. 在这种情况下, 就需要用到反向传播(BP)算法.

## BP算法一般形式的推导

首先回忆一下复合函数求导法则和多元复合函数求导法则:

$$
\begin{align}
\nabla_x f\bigl(g(x)\bigl) & = \frac{\partial f}{\partial g} \frac{\partial g}{\partial x}\\
\nabla_x f\bigl(g(x), h(x)\bigl) & = \frac{\partial f}{\partial g} \frac{\partial g}{\partial x} + 
                                     \frac{\partial f}{\partial h} \frac{\partial h}{\partial x}
\end{align}
$$

考虑如下图所示的神经元, 其中一个神经元的输出被多个不同分支的神经元作为输入(当然也可以是一个或者没有).

![一个神经元的输出被多个不同分支的神经元作为输入]()

这里的损失函数可以表示为$$f, g, h$$的多元复合函数

$$
J(\theta) = J\Bigl(g_1\bigl(f(\theta)\bigl), g_2\bigl(f(\theta)\bigl), g_3\bigl(f(\theta)\bigl)\Bigl)
          = J\Bigl(\mathbf g \bigl(f(\theta)\bigl)\Bigl)
$$

那么损失函数对参数$$\theta$$的偏导数呼之欲出

$$
\begin{align}
\nabla_f J(f) & = \frac{\partial J}{\partial g_1} \frac{\partial g_1}{\partial f} +
                  \frac{\partial J}{\partial g_2} \frac{\partial g_2}{\partial f} +
                  \frac{\partial J}{\partial g_3} \frac{\partial g_3}{\partial f}
                = \nabla_{\mathbf g} J \otimes \nabla_f \mathbf g\\
\nabla_\theta J(\theta) & = \frac{\partial J}{\partial f} \frac{\partial f}{\partial \theta}
                          = \bigl(\nabla_{\mathbf g} J \otimes \nabla_f \mathbf g \bigl) \nabla_\theta f
\end{align}            
$$

整理一下便可以得到结论

$$
\begin{align}
{\mathbf r}_f & = \nabla_{\mathbf g} J \otimes \nabla_f \mathbf g &,& \text{Residual}\\
d_f & = \nabla_\theta f &,& \text{Layer specific gradience}\\
\mathbf d & = {\mathbf r}_f d_f &,& \text{Gradience of parameter}
\end{align}
$$

![损失对一个神经元中某个参数的梯度的组成成分的可视化描述]()

## 实际BP应用举例

### 多层感知机MLP

### 前馈全连接网络

### 卷积网络

